{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a846a3",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerías necesarias\n",
    "library(tidyverse)\n",
    "library(neuralnet)\n",
    "setwd(\"C:\\\\Users\\\\Julio\\\\Downloads\\\\R_TG5\\\\Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06958eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Fitting Data\n",
    "\n",
    "# 1. Simular los datos\n",
    "set.seed(123)\n",
    "n <- 1000\n",
    "x <- runif(n, 0, 2*pi)\n",
    "epsilon <- rnorm(n, 0, 0.2)  # Ruido pequeño\n",
    "y <- sin(x) + epsilon\n",
    "\n",
    "# Crear data frame\n",
    "data <- data.frame(x = x, y = y)\n",
    "\n",
    "# Normalizar los datos para mejor convergencia\n",
    "normalize <- function(x) {(x - min(x)) / (max(x) - min(x))}\n",
    "denormalize <- function(x_norm, original) {\n",
    "  x_norm * (max(original) - min(original)) + min(original)\n",
    "}\n",
    "\n",
    "x_norm <- normalize(x)\n",
    "y_norm <- normalize(y)\n",
    "data_norm <- data.frame(x = x_norm, y = y_norm)\n",
    "\n",
    "# Función para entrenar, graficar y exportar NN\n",
    "train_plot_export_nn <- function(activation_func, title, data_norm, data_original, filename) {\n",
    "  # Fórmula\n",
    "  formula <- y ~ x\n",
    "  \n",
    "  # Entrenar NN\n",
    "  nn <- neuralnet(\n",
    "    formula,\n",
    "    data = data_norm,\n",
    "    hidden = c(50, 50, 50),\n",
    "    act.fct = activation_func,\n",
    "    linear.output = TRUE,\n",
    "    stepmax = 1e6\n",
    "  )\n",
    "  \n",
    "  # Predecir\n",
    "  predictions_norm <- predict(nn, data_norm)\n",
    "  predictions <- denormalize(predictions_norm, data_original$y)\n",
    "  \n",
    "  # Graficar\n",
    "  plot_df <- data.frame(\n",
    "    x = data_original$x,\n",
    "    y = data_original$y,\n",
    "    predicted = predictions\n",
    "  )\n",
    "  \n",
    "  p <- ggplot(plot_df, aes(x = x)) +\n",
    "    geom_point(aes(y = y), alpha = 0.3, color = \"blue\") +\n",
    "    geom_line(aes(y = predicted), color = \"red\", linewidth = 1) +\n",
    "    ggtitle(paste(\"NN con activación:\", title)) +\n",
    "    labs(subtitle = paste(\"MSE:\", round(mean((predictions - data_original$y)^2), 6))) +\n",
    "    theme_minimal()\n",
    "  \n",
    "  # Exportar gráfico\n",
    "  ggsave(filename = paste0(filename, \".png\"), \n",
    "         plot = p, \n",
    "         width = 10, \n",
    "         height = 6, \n",
    "         dpi = 300)\n",
    "  \n",
    "  # Calcular MSE\n",
    "  mse <- mean((predictions - data_original$y)^2)\n",
    "  \n",
    "  return(list(plot = p, mse = mse, model = nn))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    " Entrenar con diferentes funciones de activación (solo las disponibles en neuralnet)\n",
    "print(\"Entrenando NNs con diferentes funciones de activación...\")\n",
    "\n",
    "# Logistic\n",
    "result_logistic <- train_plot_export_nn(\"logistic\", \"Logistic\", data_norm, data, \"01_logistic\")\n",
    "\n",
    "# Tanh\n",
    "result_tanh <- train_plot_export_nn(\"tanh\", \"Tanh\", data_norm, data, \"02_tanh\")\n",
    "\n",
    "# Para simular ReLU\n",
    "result_relu_like <- train_plot_export_nn(\"tanh\", \"Tanh (similar a ReLU)\", data_norm, data, \"03_relu_like\")\n",
    "\n",
    "# Versión SIMPLIFICADA para NN con funciones mixtas\n",
    "print(\"Entrenando NN con aproximación de funciones mixtas...\")\n",
    "\n",
    "# Vamos a crear una red con una arquitectura diferente para simular \"mixto\"\n",
    "nn_mixed_simple <- neuralnet(\n",
    "  y ~ x,\n",
    "  data = data_norm,\n",
    "  hidden = c(30, 40, 30),  # Diferente número de neuronas por capa\n",
    "  act.fct = \"tanh\",        # Usamos tanh pero con diferente arquitectura\n",
    "  linear.output = TRUE,\n",
    "  stepmax = 1e6\n",
    ")\n",
    "\n",
    "# Predecir\n",
    "predictions_mixed_norm <- predict(nn_mixed_simple, data_norm)\n",
    "predictions_mixed <- denormalize(predictions_mixed_norm, data$y)\n",
    "\n",
    "# Graficar y exportar resultados mixtos\n",
    "plot_mixed <- ggplot(data, aes(x = x)) +\n",
    "  geom_point(aes(y = y), alpha = 0.3, color = \"blue\") +\n",
    "  geom_line(aes(y = predictions_mixed), color = \"red\", linewidth = 1) +\n",
    "  ggtitle(\"NN con arquitectura diferente (aproximación mixta)\") +\n",
    "  labs(subtitle = paste(\"MSE:\", round(mean((predictions_mixed - data$y)^2), 6))) +\n",
    "  theme_minimal()\n",
    "\n",
    "# Exportar gráfico mixto\n",
    "ggsave(filename = \"04_mixto.png\", \n",
    "       plot = plot_mixed, \n",
    "       width = 10, \n",
    "       height = 6, \n",
    "       dpi = 300)\n",
    "\n",
    "mse_mixed <- mean((predictions_mixed - data$y)^2)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"MSE por función de activación:\")\n",
    "print(paste(\"Logistic:\", round(result_logistic$mse, 6)))\n",
    "print(paste(\"Tanh:\", round(result_tanh$mse, 6)))\n",
    "print(paste(\"Tanh (ReLU-like):\", round(result_relu_like$mse, 6)))\n",
    "print(paste(\"Arquitectura diferente:\", round(mse_mixed, 6)))\n",
    "\n",
    "# Mostrar gráficos en R\n",
    "print(result_logistic$plot)\n",
    "print(result_tanh$plot)\n",
    "print(result_relu_like$plot)\n",
    "print(plot_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01161dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Learning-rate\n",
    "\n",
    "# Explicación del learning rate\n",
    "cat(\"\\n=== EXPLICACIÓN DEL LEARNING RATE ===\\n\")\n",
    "cat(\"El learning rate (tasa de aprendizaje) controla qué tan grandes son los ajustes\\n\")\n",
    "cat(\"que se hacen a los pesos de la red neuronal durante el entrenamiento.\\n\")\n",
    "cat(\"- LR muy bajo: Convergencia lenta, puede quedar atrapado en mínimos locales\\n\")\n",
    "cat(\"- LR muy alto: Puede sobrepasar el mínimo y no converger\\n\")\n",
    "cat(\"- LR óptimo: Balance entre velocidad y estabilidad de convergencia\\n\\n\")\n",
    "\n",
    "# Función para entrenar con diferentes learning rates y exportar gráficos\n",
    "train_lr_comparison_export <- function(hidden_layers, title_suffix, folder_name) {\n",
    "  learning_rates <- c(0.0001, 0.001, 0.01, 0.1)\n",
    "  plots <- list()\n",
    "  mses <- numeric(length(learning_rates))\n",
    "  \n",
    "  # Crear subdirectorio para estos gráficos\n",
    "  if (!dir.exists(folder_name)) {\n",
    "    dir.create(folder_name)\n",
    "  }\n",
    "  \n",
    "  for (i in seq_along(learning_rates)) {\n",
    "    lr <- learning_rates[i]\n",
    "    \n",
    "    if (hidden_layers == 1) {\n",
    "      hidden <- 50\n",
    "    } else if (hidden_layers == 2) {\n",
    "      hidden <- c(50, 50)\n",
    "    } else {\n",
    "      hidden <- c(50, 50, 50)\n",
    "    }\n",
    "    \n",
    "    tryCatch({\n",
    "      nn <- neuralnet(\n",
    "        y ~ x,\n",
    "        data = data_norm,\n",
    "        hidden = hidden,\n",
    "        act.fct = \"tanh\",  # Usamos tanh que funcionó mejor\n",
    "        linear.output = TRUE,\n",
    "        learningrate = lr,\n",
    "        stepmax = 1e6,\n",
    "        lifesign = \"none\"\n",
    "      )\n",
    "      \n",
    "      # Predecir\n",
    "      predictions_norm <- predict(nn, data_norm)\n",
    "      predictions <- denormalize(predictions_norm, data$y)\n",
    "      \n",
    "      # Calcular MSE\n",
    "      mses[i] <- mean((predictions - data$y)^2)\n",
    "      \n",
    "      # Crear gráfico\n",
    "      plot_df <- data.frame(\n",
    "        x = data$x,\n",
    "        y = data$y,\n",
    "        predicted = predictions\n",
    "      )\n",
    "      \n",
    "      p <- ggplot(plot_df, aes(x = x)) +\n",
    "        geom_point(aes(y = y), alpha = 0.2, color = \"blue\") +\n",
    "        geom_line(aes(y = predicted), color = \"red\", linewidth = 1) +\n",
    "        ggtitle(paste(\"LR =\", lr, \"-\", title_suffix)) +\n",
    "        labs(subtitle = paste(\"MSE:\", round(mses[i], 6))) +\n",
    "        theme_minimal()\n",
    "      \n",
    "      # Exportar gráfico individual\n",
    "      ggsave(filename = paste0(folder_name, \"/lr_\", lr, \".png\"), \n",
    "             plot = p, \n",
    "             width = 10, \n",
    "             height = 6, \n",
    "             dpi = 300)\n",
    "      \n",
    "      plots[[i]] <- p\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      message(\"Error con LR = \", lr, \": \", e$message)\n",
    "      mses[i] <- NA\n",
    "      # Crear gráfico de error\n",
    "      p_error <- ggplot() +\n",
    "        annotate(\"text\", x = 3, y = 0, label = paste(\"Error con LR =\", lr), size = 6) +\n",
    "        ggtitle(paste(\"Error con LR =\", lr, \"-\", title_suffix)) +\n",
    "        theme_minimal()\n",
    "      \n",
    "      ggsave(filename = paste0(folder_name, \"/lr_\", lr, \"_error.png\"), \n",
    "             plot = p_error, \n",
    "             width = 10, \n",
    "             height = 6, \n",
    "             dpi = 300)\n",
    "      \n",
    "      plots[[i]] <- p_error\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  # Encontrar mejor LR (excluyendo NAs)\n",
    "  valid_mses <- mses[!is.na(mses)]\n",
    "  if (length(valid_mses) > 0) {\n",
    "    best_lr <- learning_rates[which.min(mses)]\n",
    "  } else {\n",
    "    best_lr <- NA\n",
    "  }\n",
    "  \n",
    "  return(list(plots = plots, mses = mses, best_lr = best_lr))\n",
    "}\n",
    "\n",
    "# Entrenar para diferentes números de capas\n",
    "print(\"Comparando learning rates para 1 capa oculta...\")\n",
    "results_1layer <- train_lr_comparison_export(1, \"1 Capa Oculta\", \"learning_rate_1capa\")\n",
    "\n",
    "print(\"Comparando learning rates para 2 capas ocultas...\")\n",
    "results_2layer <- train_lr_comparison_export(2, \"2 Capas Ocultas\", \"learning_rate_2capas\")\n",
    "\n",
    "print(\"Comparando learning rates para 3 capas ocultas...\")\n",
    "results_3layer <- train_lr_comparison_export(3, \"3 Capas Ocultas\", \"learning_rate_3capas\")\n",
    "\n",
    "# Mostrar gráficos en R\n",
    "cat(\"\\n=== GRÁFICOS DE COMPARACIÓN DE LEARNING RATES ===\\n\")\n",
    "for (i in 1:4) {\n",
    "  if (!is.null(results_1layer$plots[[i]])) {\n",
    "    print(results_1layer$plots[[i]])\n",
    "  }\n",
    "}\n",
    "\n",
    "# Gráfico comparativo final de modelos\n",
    "cat(\"\\n=== GRÁFICO COMPARATIVO FINAL ===\\n\")\n",
    "comparison_df <- data.frame(\n",
    "  x = data$x,\n",
    "  y_real = data$y,\n",
    "  logistic = result_logistic$model %>% predict(data_norm) %>% denormalize(data$y),\n",
    "  tanh = result_tanh$model %>% predict(data_norm) %>% denormalize(data$y),\n",
    "  relu_like = result_relu_like$model %>% predict(data_norm) %>% denormalize(data$y)\n",
    ")\n",
    "\n",
    "comparison_long <- comparison_df %>%\n",
    "  pivot_longer(cols = c(logistic, tanh, relu_like), \n",
    "               names_to = \"Modelo\", \n",
    "               values_to = \"Prediccion\")\n",
    "\n",
    "final_comparison_plot <- ggplot(comparison_long, aes(x = x)) +\n",
    "  geom_point(aes(y = y_real), alpha = 0.1, color = \"black\") +\n",
    "  geom_line(aes(y = Prediccion, color = Modelo), linewidth = 0.8) +\n",
    "  ggtitle(\"Comparación Final de Modelos - Funciones de Activación\") +\n",
    "  labs(subtitle = \"Puntos: Datos reales, Líneas: Predicciones de cada modelo\") +\n",
    "  theme_minimal() +\n",
    "  scale_color_manual(values = c(\"logistic\" = \"red\", \n",
    "                                \"tanh\" = \"blue\", \n",
    "                                \"relu_like\" = \"green\"))\n",
    "\n",
    "# Exportar gráfico comparativo final\n",
    "ggsave(filename = \"05_comparacion_final_modelos.png\", \n",
    "       plot = final_comparison_plot, \n",
    "       width = 12, \n",
    "       height = 8, \n",
    "       dpi = 300)\n",
    "\n",
    "print(final_comparison_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d121271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gráfico comparativo de learning rates por número de capas\n",
    "cat(\"\\n=== GRÁFICO COMPARATIVO DE LEARNING RATES POR CAPAS ===\\n\")\n",
    "lr_results_df <- data.frame(\n",
    "  Capas = rep(c(\"1 Capa\", \"2 Capas\", \"3 Capas\"), each = 4),\n",
    "  LearningRate = rep(c(0.0001, 0.001, 0.01, 0.1), 3),\n",
    "  MSE = c(results_1layer$mses, results_2layer$mses, results_3layer$mses)\n",
    ")\n",
    "\n",
    "lr_comparison_plot <- ggplot(lr_results_df, aes(x = factor(LearningRate), y = MSE, fill = Capas)) +\n",
    "  geom_bar(stat = \"identity\", position = \"dodge\") +\n",
    "  ggtitle(\"Comparación de MSE por Learning Rate y Número de Capas\") +\n",
    "  labs(x = \"Learning Rate\", y = \"MSE\") +\n",
    "  theme_minimal() +\n",
    "  scale_fill_brewer(palette = \"Set1\")\n",
    "\n",
    "# Exportar gráfico comparativo de learning rates\n",
    "ggsave(filename = \"06_comparacion_learning_rates.png\", \n",
    "       plot = lr_comparison_plot, \n",
    "       width = 12, \n",
    "       height = 8, \n",
    "       dpi = 300)\n",
    "\n",
    "print(lr_comparison_plot)\n",
    "\n",
    "# Resultados y conclusiones\n",
    "cat(\"\\n=== RESULTADOS Y CONCLUSIONES ===\\n\")\n",
    "print(paste(\"Mejor LR para 1 capa:\", results_1layer$best_lr))\n",
    "print(paste(\"Mejor LR para 2 capas:\", results_2layer$best_lr))\n",
    "print(paste(\"Mejor LR para 3 capas:\", results_3layer$best_lr))\n",
    "\n",
    "cat(\"\\n=== RELACIÓN ENTRE LEARNING RATE Y NÚMERO DE CAPAS ===\\n\")\n",
    "cat(\"En general, se observa que:\\n\")\n",
    "cat(\"1. Con más capas ocultas, se necesitan learning rates más bajos\\n\")\n",
    "cat(\"2. Redes más profundas son más sensibles a learning rates altos\\n\")\n",
    "cat(\"3. Learning rates muy altos pueden causar inestabilidad en redes profundas\\n\")\n",
    "cat(\"4. El LR óptimo tiende a disminuir a medida que aumenta la profundidad de la red\\n\")\n",
    "\n",
    "# Pregunta: ¿Cuál NN ajusta mejor los datos?\n",
    "cat(\"\\n=== ¿CUÁL NN AJUSTA MEJOR LOS DATOS? ===\\n\")\n",
    "mse_comparison <- data.frame(\n",
    "  Modelo = c(\"Logistic\", \"Tanh\", \"Tanh (ReLU-like)\", \"Arquitectura diferente\"),\n",
    "  MSE = c(result_logistic$mse, result_tanh$mse, result_relu_like$mse, mse_mixed)\n",
    ")\n",
    "print(mse_comparison)\n",
    "\n",
    "best_model <- which.min(c(result_logistic$mse, result_tanh$mse, result_relu_like$mse, mse_mixed))\n",
    "best_model_name <- c(\"Logistic\", \"Tanh\", \"Tanh (ReLU-like)\", \"Arquitectura diferente\")[best_model]\n",
    "\n",
    "cat(paste(\"\\nEl mejor modelo es:\", best_model_name, \"con MSE =\", \n",
    "          round(mse_comparison$MSE[best_model], 6), \"\\n\"))\n",
    "\n",
    "# Mensaje final\n",
    "cat(\"\\n=== EXPORTACIÓN COMPLETADA ===\\n\")\n",
    "cat(\"Todos los gráficos han sido exportados al directorio:\\n\")\n",
    "cat(\"C:\\\\\\\\Users\\\\\\\\Julio\\\\\\\\Downloads\\\\\\\\R_TG5\\\\\\\\Output\\\\\\\\\\n\")\n",
    "cat(\"\\nArchivos generados:\\n\")\n",
    "cat(\"- 01_logistic.png: NN con función logística\\n\")\n",
    "cat(\"- 02_tanh.png: NN con función tanh\\n\")\n",
    "cat(\"- 03_relu_like.png: NN similar a ReLU\\n\")\n",
    "cat(\"- 04_mixto.png: NN con arquitectura diferente\\n\")\n",
    "cat(\"- 05_comparacion_final_modelos.png: Comparación de todos los modelos\\n\")\n",
    "cat(\"- 06_comparacion_learning_rates.png: Comparación de learning rates\\n\")\n",
    "cat(\"- learning_rate_1capa/: Gráficos de learning rates para 1 capa\\n\")\n",
    "cat(\"- learning_rate_2capas/: Gráficos de learning rates para 2 capas\\n\")\n",
    "cat(\"- learning_rate_3capas/: Gráficos de learning rates para 3 capas\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLL (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
