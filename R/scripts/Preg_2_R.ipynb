{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ce0a17",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerías necesarias\n",
    "library(tidyverse)\n",
    "library(glmnet)\n",
    "library(randomForest)\n",
    "library(keras)\n",
    "library(tensorflow)\n",
    "library(broom)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Cleaning and set-up\n",
    "\n",
    "# Cargar los datos\n",
    "setwd(\"C:\\\\Users\\\\Julio\\\\Downloads\\\\R_TG5\\\\Input\")\n",
    "penn_jae <- read.table(\"penn_jae.dat\", header = TRUE)\n",
    "setwd(\"C:\\\\Users\\\\Julio\\\\Downloads\\\\R_TG5\\\\Output\")\n",
    "# Ver los nombres de las variables para identificar la correcta\n",
    "print(\"Nombres de variables en el dataset:\")\n",
    "print(names(penn_jae))\n",
    "\n",
    "# Ver las primeras filas para identificar la variable de ingreso\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(head(penn_jae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a781319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtrar observaciones donde tg es 0 o 4\n",
    "data_clean <- penn_jae %>% \n",
    "  filter(tg %in% c(0, 4))\n",
    "\n",
    "# Definir variable de tratamiento T4\n",
    "data_clean <- data_clean %>% \n",
    "  mutate(T4 = ifelse(tg == 4, 1, 0))\n",
    "\n",
    "# Buscar la variable correcta para el ingreso (probablemente \"inuidur1\" o similar)\n",
    "# Si no encontramos \"inuidurl\", probamos con nombres alternativos comunes\n",
    "if (\"inuidur1\" %in% names(data_clean)) {\n",
    "  data_clean <- data_clean %>% \n",
    "    mutate(y = log(inuidur1))\n",
    "  print(\"Usando inuidur1 como variable de ingreso\")\n",
    "} else if (\"inuidur2\" %in% names(data_clean)) {\n",
    "  data_clean <- data_clean %>% \n",
    "    mutate(y = log(inuidur2))\n",
    "  print(\"Usando inuidur2 como variable de ingreso\")\n",
    "} else if (\"inuidur\" %in% names(data_clean)) {\n",
    "  data_clean <- data_clean %>% \n",
    "    mutate(y = log(inuidur))\n",
    "  print(\"Usando inuidur como variable de ingreso\")\n",
    "} else {\n",
    "  # Si no encontramos ninguna, mostramos las variables disponibles\n",
    "  print(\"Variables disponibles en el dataset:\")\n",
    "  print(names(data_clean))\n",
    "  stop(\"No se encontró la variable de ingreso. Revisa los nombres de variables arriba.\")\n",
    "}\n",
    "\n",
    "# Crear dummies para dep\n",
    "data_clean <- data_clean %>% \n",
    "  mutate(dep_0 = as.numeric(dep == 0),\n",
    "         dep_1 = as.numeric(dep == 1),\n",
    "         dep_2 = as.numeric(dep == 2))\n",
    "\n",
    "# Definir matriz x con las variables especificadas\n",
    "x_vars <- c('female', 'black', 'othrace', 'dep_1', 'dep_2', \n",
    "            'q2', 'q3', 'q4', 'q5', 'q6', 'recall', 'ageid35', \n",
    "            'ageg154', 'durable', 'nondurable', 'lusd', 'husd')\n",
    "\n",
    "# Verificar que todas las variables existen\n",
    "missing_vars <- setdiff(x_vars, names(data_clean))\n",
    "if (length(missing_vars) > 0) {\n",
    "  print(paste(\"Variables faltantes:\", paste(missing_vars, collapse = \", \")))\n",
    "  # Remover variables faltantes\n",
    "  x_vars <- intersect(x_vars, names(data_clean))\n",
    "  print(paste(\"Usando variables:\", paste(x_vars, collapse = \", \")))\n",
    "}\n",
    "\n",
    "# Crear matriz x\n",
    "x <- data_clean %>% \n",
    "  select(all_of(x_vars)) %>% \n",
    "  as.matrix()\n",
    "\n",
    "# Definir y y d\n",
    "y <- data_clean$y\n",
    "d <- data_clean$T4\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(paste(\"Dimensiones de x:\", dim(x)[1], \"filas,\", dim(x)[2], \"columnas\"))\n",
    "print(paste(\"Longitud de y:\", length(y)))\n",
    "print(paste(\"Longitud de d:\", length(d)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# II. Debiased ML\n",
    "\n",
    "# Función DML con cross-fitting\n",
    "dml <- function(y, d, x, n_folds = 5, methods_y = \"ols\", methods_d = \"ols\") {\n",
    "  n <- length(y)\n",
    "  fold_size <- floor(n / n_folds)\n",
    "  indices <- sample(1:n)\n",
    "  \n",
    "  theta_hats <- c()\n",
    "  se_hats <- c()\n",
    "  \n",
    "  for (fold in 1:n_folds) {\n",
    "    # Definir train y test para este fold\n",
    "    test_indices <- indices[((fold-1)*fold_size + 1):min(fold*fold_size, n)]\n",
    "    train_indices <- setdiff(1:n, test_indices)\n",
    "    \n",
    "    # Entrenar modelos para y y d en datos de entrenamiento\n",
    "    if (methods_y == \"ols\") {\n",
    "      model_y <- lm(y[train_indices] ~ x[train_indices, ])\n",
    "      y_hat_test <- predict(model_y, newdata = as.data.frame(x[test_indices, ]))\n",
    "    } else if (methods_y == \"lasso\") {\n",
    "      cv_lasso <- cv.glmnet(x[train_indices, ], y[train_indices], alpha = 1)\n",
    "      y_hat_test <- predict(cv_lasso, newx = x[test_indices, ], s = \"lambda.min\")\n",
    "    } else if (methods_y == \"rf\") {\n",
    "      rf_y <- randomForest(x = x[train_indices, ], y = y[train_indices])\n",
    "      y_hat_test <- predict(rf_y, newdata = x[test_indices, ])\n",
    "    }\n",
    "    \n",
    "    if (methods_d == \"ols\") {\n",
    "      model_d <- lm(d[train_indices] ~ x[train_indices, ])\n",
    "      d_hat_test <- predict(model_d, newdata = as.data.frame(x[test_indices, ]))\n",
    "    } else if (methods_d == \"lasso\") {\n",
    "      cv_lasso_d <- cv.glmnet(x[train_indices, ], d[train_indices], alpha = 1, family = \"binomial\")\n",
    "      d_hat_test <- predict(cv_lasso_d, newx = x[test_indices, ], s = \"lambda.min\", type = \"response\")\n",
    "    } else if (methods_d == \"rf\") {\n",
    "      rf_d <- randomForest(x = x[train_indices, ], y = as.factor(d[train_indices]))\n",
    "      d_hat_test <- predict(rf_d, newdata = x[test_indices, ], type = \"prob\")[,2]\n",
    "    }\n",
    "    \n",
    "    # Calcular residuos\n",
    "    v_hat <- y[test_indices] - y_hat_test\n",
    "    u_hat <- d[test_indices] - d_hat_test\n",
    "    \n",
    "    # Estimar theta para este fold\n",
    "    theta_fold <- mean(u_hat * v_hat) / mean(u_hat * d[test_indices])\n",
    "    theta_hats[fold] <- theta_fold\n",
    "    \n",
    "    # Calcular error estándar\n",
    "    se_fold <- sd(u_hat * v_hat) / (sqrt(length(test_indices)) * abs(mean(u_hat * d[test_indices])))\n",
    "    se_hats[fold] <- se_fold\n",
    "  }\n",
    "  \n",
    "  # Promediar estimaciones entre folds\n",
    "  theta_hat <- mean(theta_hats)\n",
    "  se_hat <- mean(se_hats)\n",
    "  \n",
    "  return(list(theta_hat = theta_hat, se_hat = se_hat, theta_hats = theta_hats))\n",
    "}\n",
    "\n",
    "# Estimar con diferentes métodos\n",
    "results_dml <- data.frame()\n",
    "\n",
    "# OLS\n",
    "dml_ols <- dml(y, d, x, methods_y = \"ols\", methods_d = \"ols\")\n",
    "results_dml <- rbind(results_dml, \n",
    "                     data.frame(Method = \"OLS\", \n",
    "                                Estimate = dml_ols$theta_hat,\n",
    "                                SE = dml_ols$se_hat))\n",
    "\n",
    "# Lasso\n",
    "dml_lasso <- dml(y, d, x, methods_y = \"lasso\", methods_d = \"lasso\")\n",
    "results_dml <- rbind(results_dml, \n",
    "                     data.frame(Method = \"Lasso\", \n",
    "                                Estimate = dml_lasso$theta_hat,\n",
    "                                SE = dml_lasso$se_hat))\n",
    "\n",
    "# Random Forest\n",
    "dml_rf <- dml(y, d, x, methods_y = \"rf\", methods_d = \"rf\")\n",
    "results_dml <- rbind(results_dml, \n",
    "                     data.frame(Method = \"Random Forest\", \n",
    "                                Estimate = dml_rf$theta_hat,\n",
    "                                SE = dml_rf$se_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network (simplificado - en práctica necesitarías ajustar más)\n",
    "tryCatch({\n",
    "  # Normalizar datos para NN\n",
    "  x_scaled <- scale(x)\n",
    "  \n",
    "  dml_nn <- dml(y, d, x_scaled, methods_y = \"nn\", methods_d = \"nn\")\n",
    "  results_dml <- rbind(results_dml, \n",
    "                       data.frame(Method = \"Neural Network\", \n",
    "                                  Estimate = dml_nn$theta_hat,\n",
    "                                  SE = dml_nn$se_hat))\n",
    "}, error = function(e) {\n",
    "  message(\"NN no disponible: \", e$message)\n",
    "})\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados DML con Cross-Fitting:\")\n",
    "print(results_dml)\n",
    "\n",
    "# III. No cross-fitting\n",
    "\n",
    "# Función sin cross-fitting\n",
    "dml_no_cf <- function(y, d, x, methods_y = \"ols\", methods_d = \"ols\") {\n",
    "  n <- length(y)\n",
    "  \n",
    "  # Dividir datos una sola vez\n",
    "  train_indices <- sample(1:n, size = floor(0.7 * n))\n",
    "  test_indices <- setdiff(1:n, train_indices)\n",
    "  \n",
    "  # Entrenar modelos para y y d\n",
    "  if (methods_y == \"ols\") {\n",
    "    model_y <- lm(y[train_indices] ~ x[train_indices, ])\n",
    "    y_hat_test <- predict(model_y, newdata = as.data.frame(x[test_indices, ]))\n",
    "    y_hat_train <- predict(model_y, newdata = as.data.frame(x[train_indices, ]))\n",
    "  } else if (methods_y == \"lasso\") {\n",
    "    cv_lasso <- cv.glmnet(x[train_indices, ], y[train_indices], alpha = 1)\n",
    "    y_hat_test <- predict(cv_lasso, newx = x[test_indices, ], s = \"lambda.min\")\n",
    "    y_hat_train <- predict(cv_lasso, newx = x[train_indices, ], s = \"lambda.min\")\n",
    "  } else if (methods_y == \"rf\") {\n",
    "    rf_y <- randomForest(x = x[train_indices, ], y = y[train_indices])\n",
    "    y_hat_test <- predict(rf_y, newdata = x[test_indices, ])\n",
    "    y_hat_train <- predict(rf_y, newdata = x[train_indices, ])\n",
    "  }\n",
    "  \n",
    "  if (methods_d == \"ols\") {\n",
    "    model_d <- lm(d[train_indices] ~ x[train_indices, ])\n",
    "    d_hat_test <- predict(model_d, newdata = as.data.frame(x[test_indices, ]))\n",
    "    d_hat_train <- predict(model_d, newdata = as.data.frame(x[train_indices, ]))\n",
    "  } else if (methods_d == \"lasso\") {\n",
    "    cv_lasso_d <- cv.glmnet(x[train_indices, ], d[train_indices], alpha = 1, family = \"binomial\")\n",
    "    d_hat_test <- predict(cv_lasso_d, newx = x[test_indices, ], s = \"lambda.min\", type = \"response\")\n",
    "    d_hat_train <- predict(cv_lasso_d, newx = x[train_indices, ], s = \"lambda.min\", type = \"response\")\n",
    "  } else if (methods_d == \"rf\") {\n",
    "    rf_d <- randomForest(x = x[train_indices, ], y = as.factor(d[train_indices]))\n",
    "    d_hat_test <- predict(rf_d, newdata = x[test_indices, ], type = \"prob\")[,2]\n",
    "    d_hat_train <- predict(rf_d, newdata = x[train_indices, ], type = \"prob\")[,2]\n",
    "  }\n",
    "  \n",
    "  # Calcular RMSE\n",
    "  rmse_y_test <- sqrt(mean((y[test_indices] - y_hat_test)^2))\n",
    "  rmse_d_test <- sqrt(mean((d[test_indices] - d_hat_test)^2))\n",
    "  rmse_y_train <- sqrt(mean((y[train_indices] - y_hat_train)^2))\n",
    "  rmse_d_train <- sqrt(mean((d[train_indices] - d_hat_train)^2))\n",
    "  \n",
    "  # Calcular residuos para test\n",
    "  v_hat <- y[test_indices] - y_hat_test\n",
    "  u_hat <- d[test_indices] - d_hat_test\n",
    "  \n",
    "  # Estimar theta\n",
    "  theta_hat <- mean(u_hat * v_hat) / mean(u_hat * d[test_indices])\n",
    "  se_hat <- sd(u_hat * v_hat) / (sqrt(length(test_indices)) * abs(mean(u_hat * d[test_indices])))\n",
    "  \n",
    "  return(list(theta_hat = theta_hat, \n",
    "              se_hat = se_hat,\n",
    "              rmse_y_test = rmse_y_test,\n",
    "              rmse_d_test = rmse_d_test,\n",
    "              rmse_y_train = rmse_y_train,\n",
    "              rmse_d_train = rmse_d_train))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimar sin cross-fitting\n",
    "results_no_cf <- data.frame()\n",
    "\n",
    "# OLS sin CF\n",
    "dml_ols_nocf <- dml_no_cf(y, d, x, methods_y = \"ols\", methods_d = \"ols\")\n",
    "results_no_cf <- rbind(results_no_cf, \n",
    "                       data.frame(Method = \"OLS\", \n",
    "                                  Estimate = dml_ols_nocf$theta_hat,\n",
    "                                  SE = dml_ols_nocf$se_hat,\n",
    "                                  RMSE_y = dml_ols_nocf$rmse_y_test,\n",
    "                                  RMSE_d = dml_ols_nocf$rmse_d_test))\n",
    "\n",
    "# Lasso sin CF\n",
    "dml_lasso_nocf <- dml_no_cf(y, d, x, methods_y = \"lasso\", methods_d = \"lasso\")\n",
    "results_no_cf <- rbind(results_no_cf, \n",
    "                       data.frame(Method = \"Lasso\", \n",
    "                                  Estimate = dml_lasso_nocf$theta_hat,\n",
    "                                  SE = dml_lasso_nocf$se_hat,\n",
    "                                  RMSE_y = dml_lasso_nocf$rmse_y_test,\n",
    "                                  RMSE_d = dml_lasso_nocf$rmse_d_test))\n",
    "\n",
    "# RF sin CF\n",
    "dml_rf_nocf <- dml_no_cf(y, d, x, methods_y = \"rf\", methods_d = \"rf\")\n",
    "results_no_cf <- rbind(results_no_cf, \n",
    "                       data.frame(Method = \"Random Forest\", \n",
    "                                  Estimate = dml_rf_nocf$theta_hat,\n",
    "                                  SE = dml_rf_nocf$se_hat,\n",
    "                                  RMSE_y = dml_rf_nocf$rmse_y_test,\n",
    "                                  RMSE_d = dml_rf_nocf$rmse_d_test))\n",
    "\n",
    "print(\"Resultados sin Cross-Fitting:\")\n",
    "print(results_no_cf)\n",
    "\n",
    "# Comparación de resultados\n",
    "comparison <- merge(results_dml, results_no_cf, by = \"Method\", suffixes = c(\"_CF\", \"_NoCF\"))\n",
    "print(\"Comparación entre métodos:\")\n",
    "print(comparison)\n",
    "\n",
    "# Respuestas a las preguntas\n",
    "cat(\"\\n=== RESPUESTAS A LAS PREGUNTAS ===\\n\")\n",
    "cat(\"1. El RMSE para predecir y y d generalmente será menor sin cross-fitting\\n\")\n",
    "cat(\"   porque los modelos se sobreajustan a los datos de entrenamiento.\\n\\n\")\n",
    "cat(\"2. Sin cross-fitting obtenemos RMSE más bajos porque evaluamos en los\\n\") \n",
    "cat(\"   mismos datos usados para entrenar, lo que no refleja el desempeño real.\\n\\n\")\n",
    "cat(\"3. El problema principal sin cross-fitting es el overfitting y\\n\")\n",
    "cat(\"   la estimación sesgada del efecto causal debido a la contaminación\\n\")\n",
    "cat(\"   entre datos de entrenamiento y prueba.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLL (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
